{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6eb145",
   "metadata": {},
   "source": [
    "# EUR-LEX Universal Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46889348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d96cff",
   "metadata": {},
   "source": [
    "### Opening and access the HTML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a3c7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For opening the raw HTML file we declare the path here as a class.\n",
    "html_file = Path(\"../data/regulations/32017R0745/EN/raw/raw.html\")\n",
    "\n",
    "# Then we read the content of the file into a variable.\n",
    "raw_html = html_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# We parse the raw HTML content using BeautifulSoup to create a navigable tree structure.\n",
    "soup = BeautifulSoup(raw_html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316557a",
   "metadata": {},
   "source": [
    "Up to now we have just used the `Path` class from the library `Pathlib` to create a path object, which then we used to open the `raw_html` through the `read_text()` method. Then the `soup` variable was created which contains a navigable tree structure thanks to html tagged structure. Printing the `soup` variable will output the full html mess "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50164d82",
   "metadata": {},
   "source": [
    "According to BeautifulSoup documentation, the method `find_all()` looks through a tag's descendants and retrieves all descendants that match all our filters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15821cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"h1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_eu_ai_act_html(html_content: str, celex: str = \"32024R1689\") -> Dict:\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "    provisions = []\n",
    "    relations = []\n",
    "    id_to_node = {}          # for building parent_id quickly\n",
    "\n",
    "    # 1. Find main content\n",
    "    main = soup.find(\"div\", id=\"docHtml\") or soup.find(\"div\", class_=\"eli-container\")\n",
    "\n",
    "    # 2. Recursive walker (depth-first, respects DOM nesting)\n",
    "    def walk(element, parent_id=None, current_path=None):\n",
    "        if not element or not element.get(\"id\"):\n",
    "            return\n",
    "\n",
    "        node_id = element[\"id\"]\n",
    "\n",
    "        # Determine kind from ID prefix (official ELI)\n",
    "        if re.match(r\"rct_\\d+\", node_id):\n",
    "            kind = \"recital\"\n",
    "            number = node_id.split(\"_\")[1]\n",
    "            full_id = f\"{celex}_rec_{number}\"\n",
    "            citation = f\"Recital ({number})\"\n",
    "\n",
    "        elif re.match(r\"cpt_[IVX]+\", node_id):\n",
    "            kind = \"chapter\"\n",
    "            number = node_id.split(\"_\")[1]\n",
    "            full_id = f\"{celex}_cpt_{number}\"\n",
    "            citation = f\"Chapter {number}\"\n",
    "\n",
    "        elif re.search(r\"\\.sct_\\d+\", node_id):\n",
    "            kind = \"section\"\n",
    "            # extract section number\n",
    "            number = re.search(r\"sct_(\\d+)\", node_id).group(1)\n",
    "            full_id = f\"{celex}_sct_{number}\"\n",
    "            citation = f\"Section {number}\"\n",
    "\n",
    "        elif re.match(r\"art_\\d+\", node_id):\n",
    "            kind = \"article\"\n",
    "            number = node_id.split(\"_\")[1]\n",
    "            full_id = f\"{celex}_art_{number}\"\n",
    "            citation = f\"Article {number}\"\n",
    "\n",
    "        elif re.match(r\"anx_[IVX0-9]+\", node_id):\n",
    "            kind = \"annex\"\n",
    "            number = node_id.split(\"_\")[1]\n",
    "            full_id = f\"{celex}_anx_{number}\"\n",
    "            citation = f\"Annex {number}\"\n",
    "\n",
    "        else:\n",
    "            # fallback for paragraphs/points inside tables\n",
    "            kind = \"paragraph\"   # will be refined later\n",
    "            full_id = f\"{celex}_{node_id}\"\n",
    "            citation = \"\"\n",
    "\n",
    "        # Extract text (handle the classic EUR-Lex table pattern)\n",
    "        text = \"\"\n",
    "        table = element.find(\"table\")\n",
    "        if table:\n",
    "            rows = table.find_all(\"tr\")\n",
    "            for row in rows:\n",
    "                cells = row.find_all(\"td\")\n",
    "                if len(cells) >= 2:\n",
    "                    number_cell = cells[0].get_text(strip=True)\n",
    "                    text_cell = cells[1].get_text(separator=\" \", strip=True)\n",
    "                    text += f\"{number_cell} {text_cell}\\n\"\n",
    "        else:\n",
    "            text = element.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        # Build node\n",
    "        node = {\n",
    "            \"id\": node_id,\n",
    "            \"canonical_id\": full_id,\n",
    "            \"celex\": celex,\n",
    "            \"regulation_id\": \"EU AI Act\",\n",
    "            \"lang\": \"EN\",\n",
    "            \"kind\": kind,\n",
    "            \"level\": kind,                     # or derive from depth\n",
    "            \"item_number\": number if 'number' in locals() else None,\n",
    "            \"title\": element.find([\"h1\",\"h2\",\"h3\",\"strong\"]).get_text(strip=True) if element.find([\"h1\",\"h2\",\"h3\",\"strong\"]) else None,\n",
    "            \"text\": text.strip(),\n",
    "            \"path\": current_path or [],\n",
    "            \"parent_id\": parent_id,\n",
    "            \"depth\": len(current_path) if current_path else 0,\n",
    "            # ... all your other fields (provenance, snippet, etc.)\n",
    "            \"is_requirement\": False,   # LLM step 2\n",
    "            \"requirement_type\": None,\n",
    "            \"obligations\": [],\n",
    "            \"references\": [],\n",
    "            \"roles\": [],\n",
    "            \"provenance\": { ... }      # html_start, raw_hash, etc.\n",
    "        }\n",
    "\n",
    "        provisions.append(node)\n",
    "        id_to_node[full_id] = node\n",
    "\n",
    "        # Recurse into children\n",
    "        new_path = (current_path or []) + [full_id]\n",
    "        for child in element.find_all(\"div\", class_=\"eli-subdivision\", recursive=False):\n",
    "            walk(child, parent_id=full_id, current_path=new_path)\n",
    "\n",
    "    # Start walking from enacting terms and annexes\n",
    "    for top_level in main.find_all([\"div\"], id=re.compile(r\"(rct_|cpt_|art_|anx_)\")):\n",
    "        walk(top_level)\n",
    "\n",
    "    return {\n",
    "        \"graph_version\": \"0.2\",\n",
    "        \"celex_id\": celex,\n",
    "        \"regulation_id\": \"EU AI Act\",\n",
    "        \"source_name\": \"EUR-Lex HTML\",\n",
    "        \"generated_at\": \"...\",\n",
    "        \"provisions\": provisions,\n",
    "        \"relations\": relations   # built from detected \"Article XX\", \"Annex Y\" etc.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6edfd15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crss_mvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
